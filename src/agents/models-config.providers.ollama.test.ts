import { mkdtempSync } from "node:fs";
import { tmpdir } from "node:os";
import { join } from "node:path";
import { describe, expect, it } from "vitest";
import { normalizeProviders, resolveImplicitProviders } from "./models-config.providers.js";

describe("Ollama provider", () => {
  it("should not include ollama when no API key is configured", async () => {
    const agentDir = mkdtempSync(join(tmpdir(), "openclaw-test-"));
    const providers = await resolveImplicitProviders({ agentDir });

    // Ollama requires explicit configuration via OLLAMA_API_KEY env var or profile
    expect(providers?.ollama).toBeUndefined();
  });

  it("should disable streaming by default for Ollama models", async () => {
    const agentDir = mkdtempSync(join(tmpdir(), "openclaw-test-"));
    process.env.OLLAMA_API_KEY = "test-key";

    try {
      const providers = await resolveImplicitProviders({ agentDir });

      // Provider should be defined with OLLAMA_API_KEY set
      expect(providers?.ollama).toBeDefined();
      expect(providers?.ollama?.apiKey).toBe("OLLAMA_API_KEY");

      // Note: discoverOllamaModels() returns empty array in test environments (VITEST env var check)
      // so we can't test the actual model discovery here. The streaming: false setting
      // is applied in the model mapping within discoverOllamaModels().
      // The configuration structure itself is validated by TypeScript and the Zod schema.
    } finally {
      delete process.env.OLLAMA_API_KEY;
    }
  });

  it("should have correct model structure with streaming disabled (unit test)", () => {
    // This test directly verifies the model configuration structure
    // since discoverOllamaModels() returns empty array in test mode
    const mockOllamaModel = {
      id: "llama3.3:latest",
      name: "llama3.3:latest",
      reasoning: false,
      input: ["text"],
      cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
      contextWindow: 128000,
      maxTokens: 8192,
      params: {
        streaming: false,
      },
    };

    // Verify the model structure matches what discoverOllamaModels() would return
    expect(mockOllamaModel.params?.streaming).toBe(false);
    expect(mockOllamaModel.params).toHaveProperty("streaming");
  });

  it("should inject streaming:false on explicit Ollama providers via normalizeProviders", () => {
    const agentDir = mkdtempSync(join(tmpdir(), "openclaw-test-"));
    // Simulate the explicit config generated by setup scripts
    const providers = {
      local_ollama: {
        baseUrl: "http://127.0.0.1:11434/v1",
        api: "openai-completions" as const,
        apiKey: "ollama",
        models: [
          {
            id: "llama3",
            name: "Llama 3",
            reasoning: false,
            input: ["text"] as Array<"text" | "image">,
            cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
            contextWindow: 128000,
            maxTokens: 8192,
          },
        ],
      },
    };

    const normalized = normalizeProviders({ providers, agentDir });
    const model = normalized?.local_ollama?.models?.[0] as Record<string, unknown>;
    expect(model).toBeDefined();
    const params = model.params as Record<string, unknown>;
    expect(params).toBeDefined();
    expect(params.streaming).toBe(false);
  });

  it("should detect Ollama by port even with custom provider name", () => {
    const agentDir = mkdtempSync(join(tmpdir(), "openclaw-test-"));
    const providers = {
      my_local_llm: {
        baseUrl: "http://localhost:11434/v1",
        api: "openai-completions" as const,
        apiKey: "test",
        models: [
          {
            id: "mistral",
            name: "Mistral",
            reasoning: false,
            input: ["text"] as Array<"text" | "image">,
            cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
            contextWindow: 128000,
            maxTokens: 8192,
          },
        ],
      },
    };

    const normalized = normalizeProviders({ providers, agentDir });
    const model = normalized?.my_local_llm?.models?.[0] as Record<string, unknown>;
    const params = model.params as Record<string, unknown>;
    expect(params).toBeDefined();
    expect(params.streaming).toBe(false);
  });

  it("should not override explicit streaming:true on Ollama models", () => {
    const agentDir = mkdtempSync(join(tmpdir(), "openclaw-test-"));
    const providers = {
      local_ollama: {
        baseUrl: "http://127.0.0.1:11434/v1",
        api: "openai-completions" as const,
        apiKey: "ollama",
        models: [
          {
            id: "llama3",
            name: "Llama 3",
            reasoning: false,
            input: ["text"] as Array<"text" | "image">,
            cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
            contextWindow: 128000,
            maxTokens: 8192,
            // User explicitly enabled streaming (e.g. fixed Ollama version)
            params: { streaming: true },
          } as Record<string, unknown>,
        ],
      },
    };

    const normalized = normalizeProviders({ providers: providers as never, agentDir });
    const model = normalized?.local_ollama?.models?.[0] as Record<string, unknown>;
    const params = model.params as Record<string, unknown>;
    expect(params.streaming).toBe(true);
  });
});
